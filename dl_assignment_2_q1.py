# -*- coding: utf-8 -*-
"""DL_Assignment_2_Q1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HonmtTc4RNjxiZs9aneXaEjsfa_eauzj
"""

from google.colab import drive

drive.mount("/content/gdrive")

root_path = "gdrive/My Drive/"
import pandas as pd

import numpy as np

"""###Code for Reading images"""


import os

files = os.listdir("gdrive/My Drive/DL_Assignment/Data_Set_1(Colored_Images)/street/")
len(files)

image = []
for f in files:
    image.append(
        np.loadtxt(
            "gdrive/My Drive/DL_Assignment/Data_Set_1(Colored_Images)/street/" + f
        )
    )

street = np.asarray(image)

insidecity.shape

street = street.reshape(292, 828)

np.savetxt(
    "gdrive/My Drive/DL_Assignment/Data_Set_1(Colored_Images)/street.csv",
    street,
    delimiter=",",
)

import pandas as pd
from numpy import genfromtxt
import numpy as np

images = np.vstack((images, street))

street = genfromtxt(
    "gdrive/My Drive/DL_Assignment/Data_Set_1(Colored_Images)/street.csv", delimiter=","
)

"""###PCA"""

import pandas as pd
from numpy import genfromtxt
import numpy as np

images = genfromtxt(
    "gdrive/My Drive/DL_Assignment/Data_Set_1(Colored_Images)/images.csv", delimiter=","
)

true_labels = genfromtxt(
    "gdrive/My Drive/DL_Assignment/Data_Set_1(Colored_Images)/true_labels.csv",
    delimiter=",",
)

import sklearn

from sklearn.decomposition import PCA

var = []
for i in range(200):
    pca = PCA(n_components=i)
    img_red = pca.fit_transform(images)
    arr = pca.explained_variance_ratio_
    var.append(sum(arr))

import matplotlib.pyplot as plt

plt.plot(range(1, 201), var)
plt.title("Scree Plot")
plt.ylabel("Variance Captured")
plt.xlabel("Number of Components")

var[144]

pca = PCA(n_components=300)
img_red = pca.fit_transform(images)

img_red.shape

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras as keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

from sklearn.model_selection import train_test_split
from keras.callbacks import LearningRateScheduler

X_train, X_val, y_train, y_val = train_test_split(
    img_red, true_labels, test_size=0.2, random_state=1234
)

reg_val = 0.1
inputs = tf.keras.Input(shape=(300,))
x = tf.keras.layers.Dense(
    80,
    activation="relu",
    kernel_regularizer=tf.keras.regularizers.l2(reg_val),
    bias_regularizer=tf.keras.regularizers.l2(reg_val),
)(inputs)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(
    30,
    activation="relu",
    kernel_regularizer=tf.keras.regularizers.l2(reg_val),
    bias_regularizer=tf.keras.regularizers.l2(reg_val),
)(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = tf.keras.layers.Dense(5, activation="softmax")(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs, name="Task_1_part_1")
# callback = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=4)
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(lr=1e-4),
    metrics=["accuracy"],
)
history = model.fit(
    X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_val, y_val)
)

# use the keras functional api
reg_val = 0
X_train, X_val, y_train, y_val = train_test_split(
    img_red, true_labels, test_size=0.2, random_state=1234
)
inputs = tf.keras.Input(shape=(300,))
x = tf.keras.layers.Dense(
    80,
    activation="relu",
    kernel_regularizer=tf.keras.regularizers.l2(reg_val),
    bias_regularizer=tf.keras.regularizers.l2(reg_val),
)(inputs)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(
    70,
    activation="relu",
    kernel_regularizer=tf.keras.regularizers.l2(reg_val),
    bias_regularizer=tf.keras.regularizers.l2(reg_val),
)(x)
x = tf.keras.layers.Dropout(0.2)(x)
outputs = tf.keras.layers.Dense(5)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs, name="Task_1_part_1")
callback = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=4)
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(1e-4),
    metrics=["accuracy"],
)
history = model.fit(
    X_train,
    y_train,
    epochs=3000,
    batch_size=128,
    callbacks=[callback],
    validation_data=(X_val, y_val),
    verbose=0,
)

# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.title("model accuracy")
plt.ylabel("accuracy")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

# summarize history for loss
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

# let's estimate our result
scores = model.evaluate(X_val, y_val)
print("Test loss:", scores[0])
print("Test accuracy:", scores[1])

"""### AutoEncoders"""

from keras.layers import Input, Dense, Dropout
from keras.models import Model

encoding_dim = (
    1400  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats
)
reg_val = 1e-8
# this is our input placeholder
input_img = Input(shape=(828,))
# "encoded" is the encoded representation of the input
encoded = Dense(
    encoding_dim,
    activation="tanh",
    kernel_regularizer=tf.keras.regularizers.l1(reg_val),
    bias_regularizer=tf.keras.regularizers.l1(reg_val),
)(input_img)
# "decoded" is the lossy reconstruction of the input
# x = Dropout(0.2)(encoded)
decoded = Dense(828)(encoded)

# this model maps an input to its reconstruction
autoencoder = Model(input_img, decoded)

encoder = Model(input_img, encoded)

# create a placeholder for an encoded (32-dimensional) input
encoded_input = Input(shape=(encoding_dim,))
# retrieve the last layer of the autoencoder model
decoder_layer = autoencoder.layers[-1]
# create the decoder model
decoder = Model(encoded_input, decoder_layer(encoded_input))

autoencoder.compile(
    loss=tf.keras.losses.MeanSquaredError(),
    optimizer=tf.keras.optimizers.Adam(1e-4),
    metrics=["mse"],
)

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import normalize

# images = normalize(images)

images = genfromtxt(
    "gdrive/My Drive/DL_Assignment/Data_Set_1(Colored_Images)/images.csv", delimiter=","
)

X_train, X_val, y_train, y_val = train_test_split(
    images, images, test_size=0.2, random_state=1234
)
history = autoencoder.fit(
    X_train,
    y_train,
    epochs=300,
    batch_size=32,
    validation_data=(X_val, y_val),
    verbose=0,
)

# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history["mse"])
plt.plot(history.history["val_mse"])
plt.title("autoencoder MSE")
plt.ylabel("MSE")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

# summarize history for loss
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("autoencoder loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

scores = autoencoder.evaluate(X_val, X_val, verbose=1)
print("Test loss:", scores[0])
print("Test MSE:", scores[1])

encoded_imgs = encoder.predict(images)

encoded_imgs.shape

true_labels

X_train, X_val, y_train, y_val = train_test_split(
    encoded_imgs, true_labels, test_size=0.2, random_state=1234
)
reg_val = 0
inputs = tf.keras.Input(shape=(1400,))
x = tf.keras.layers.Dense(
    72,
    activation="relu",
    kernel_regularizer=tf.keras.regularizers.l2(reg_val),
    bias_regularizer=tf.keras.regularizers.l2(reg_val),
)(inputs)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(
    36,
    activation="relu",
    kernel_regularizer=tf.keras.regularizers.l2(reg_val),
    bias_regularizer=tf.keras.regularizers.l2(reg_val),
)(x)
x = tf.keras.layers.Dropout(0.2)(x)
# x = tf.keras.layers.Dense(100,activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_val), bias_regularizer=tf.keras.regularizers.l2(reg_val))(x)
# x = tf.keras.layers.Dropout(0.2)(x)
# x = tf.keras.layers.Dense(50,activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_val), bias_regularizer=tf.keras.regularizers.l2(reg_val))(x)
# x = tf.keras.layers.Dropout(0.2)(x)
# x = tf.keras.layers.Dense(25,activation="relu", kernel_regularizer=tf.keras.regularizers.l2(reg_val), bias_regularizer=tf.keras.regularizers.l2(reg_val))(x)
# x = tf.keras.layers.Dropout(0.2)(x)
outputs = tf.keras.layers.Dense(5, activation="softmax")(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs, name="Task_1_part_1")
callback = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=4, verbose=1)
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(1e-5),
    metrics=["accuracy"],
)
history = model.fit(
    X_train,
    y_train,
    epochs=1000,
    batch_size=32,
    validation_data=(X_val, y_val),
    verbose=0,
)

# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.title("model accuracy")
plt.ylabel("accuracy")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

# summarize history for loss
plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

scores = model.evaluate(X_val, y_val, verbose=1)
print("Test loss:", scores[0])
print("Test accuracy:", scores[1])

images
