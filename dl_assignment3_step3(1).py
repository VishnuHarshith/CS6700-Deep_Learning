# -*- coding: utf-8 -*-
"""DL_Assignment3_Step3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12pe4prb--ONv_iOFEriwQMcnMP-dQ3jX
"""

from google.colab import drive

drive.mount("/content/gdrive")

import pandas as pd
import numpy as np
from numpy import genfromtxt
import cv2

# data_path = '/content/gdrive/My Drive/Images_Round2_Class21/'
# # we'll use data from two folders
shelf_images = "/content/gdrive/My Drive/DL_Assignment_3/CUB_200_2011/images/"
# product_images = '/content/gdrive/My Drive/Images_Round2_Class21/Dataset_Final_Class_21/p'

vgg_test = np.load("/content/gdrive/My Drive/DL_Assignment_3/vggtest.npy")

vgg_train = np.load("/content/gdrive/My Drive/DL_Assignment_3/vggtrain.npy")

vgg_val = np.load("/content/gdrive/My Drive/DL_Assignment_3/vggval.npy")

gnet_train = np.load("/content/gdrive/My Drive/DL_Assignment_3/gnettrain.npy")

gnet_val = np.load("/content/gdrive/My Drive/DL_Assignment_3/gnetval.npy")

gnet_test = np.load("/content/gdrive/My Drive/DL_Assignment_3/gnettest.npy")

gnet_val.shape

# load data from previous step
train_data = pd.read_csv(
    "/content/gdrive/My Drive/DL_Assignment_3/train_data_team43.csv"
)

val_data = pd.read_csv(
    "/content/gdrive/My Drive/DL_Assignment_3/validation_data_team43.csv"
)

test_data = pd.read_csv("/content/gdrive/My Drive/DL_Assignment_3/test_data_team43.csv")

y = train_data.label.values
for i in range(len(y)):
    if y[i] == 20:
        y[i] = 0
    elif y[i] == 41:
        y[i] = 1
    elif y[i] == 95:
        y[i] = 2
    elif y[i] == 114:
        y[i] = 3
    elif y[i] == 128:
        y[i] = 4
    elif y[i] == 131:
        y[i] = 5
    elif y[i] == 135:
        y[i] = 6

y_train = np.array(y)

y = test_data.label.values
for i in range(len(y)):
    if y[i] == 20:
        y[i] = 0
    elif y[i] == 41:
        y[i] = 1
    elif y[i] == 95:
        y[i] = 2
    elif y[i] == 114:
        y[i] = 3
    elif y[i] == 128:
        y[i] = 4
    elif y[i] == 131:
        y[i] = 5
    elif y[i] == 135:
        y[i] = 6

y_test = np.array(y)

y = val_data.label.values
for i in range(len(y)):
    if y[i] == 20:
        y[i] = 0
    elif y[i] == 41:
        y[i] = 1
    elif y[i] == 95:
        y[i] = 2
    elif y[i] == 114:
        y[i] = 3
    elif y[i] == 128:
        y[i] = 4
    elif y[i] == 131:
        y[i] = 5
    elif y[i] == 135:
        y[i] = 6

y_validation = np.array(y)

gnet_train.shape

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras as keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

num_classes = 7

# convert y_train and y_validation to one-hot arrays
y_train = keras.utils.to_categorical(y_train, num_classes)
y_validation = keras.utils.to_categorical(y_validation, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPool2D, Flatten
from keras.preprocessing.image import ImageDataGenerator
import numpy as np
from tensorflow.keras import Input

gnet_train.shape

import tensorflow as tf

model = tf.keras.Sequential()
model.add(Input(shape=(8, 8, 2048,)))
model.add(tf.keras.layers.MaxPool2D(pool_size=(6, 6), strides=(2, 2)))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(units=4096, activation="relu"))
model.add(tf.keras.layers.Dense(units=2048, activation="relu"))
# model.add(tf.keras.layers.Dense(units=1000,activation="relu"))
model.add(tf.keras.layers.Dense(units=7, activation="softmax"))

model.summary()

from tensorflow.keras.optimizers import Adam, SGD

opt = Adam(lr=0.001)
model.compile(
    optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=["accuracy"]
)

history = model.fit(
    x=gnet_train,
    y=y_train,
    validation_data=(gnet_val, y_validation),
    verbose=1,
    epochs=500,
)

# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.title("model accuracy")
plt.ylabel("accuracy")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

model.save("/content/gdrive/My Drive/DL_Assignment_3/")

scores = model.evaluate(gnet_test, y_test, verbose=1)
print("Test loss:", scores[0])
print("Test accuracy:", scores[1])

# let's draw confusion matrix to check classes recognition performance
y_validation_cls = np.argmax(y_train, axis=1)
y_validation_predict = model.predict(gnet_train)
y_validation_predict_cls = np.argmax(y_validation_predict, axis=1)

from sklearn.metrics import confusion_matrix, plot_confusion_matrix

import seaborn as sns

fig = plt.gcf()
# fig.set_size_inches(10, 10)
cnf_matrix = confusion_matrix(y_validation_cls, y_validation_predict_cls)

sns.heatmap(cnf_matrix, annot=True, annot_kws={"size": 16})

from keras.utils.vis_utils import plot_model

plot_model(model, to_file="model_plot.png", show_shapes=True, show_layer_names=True)

import pickle

filename = "/content/gdrive/My Drive/DL_Assignment_3/vggpred.sav"
pickle.dump(model, open(filename, "wb"))

from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    featurewise_center=False,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
)
# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)

# fits the model on batches with real-time data augmentation:
# model.fit_generator(datagen.flow(gnet_train, y_train, batch_size=32),
#                     steps_per_epoch=len(x_train) / 32, epochs=epochs, val)

datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=False,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=False,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=False,  # randomly flip images
    vertical_flip=False,
)  # randomly flip images
datagen.fit(gnet_train)
